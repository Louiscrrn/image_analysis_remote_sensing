{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SICOM-SIGMA SEGMENTATION - RECOMPOSITION 2024/2025\n",
    "In this lab we will process and play around with a famous State-of-the-Art detection model : YOLO (You Look Only Once)\n",
    "This Lab will be less guided and will challenge your autonomy and curiosity.\n",
    "\n",
    "## Goals\n",
    "\n",
    "The expected result is either this Notebook with the final images (The first mission is enough, the 2 other are bonuses) displayed at the end, either a python code file/folder and the pictures zipped together\n",
    "\n",
    "**First mission:**\n",
    "\n",
    "Given a dataset both in visible and infrared modalities (10 images each), find and cut out from the images all the people (30 ~ 60) and their bicycles/moto and sent them to Mars ('/data/planetary stages/mars.jpg')\n",
    "\n",
    "    bonuses:\n",
    "        - Adapt their respective Hue to match the vibe on Mars\n",
    "        - Scale their size on the distance, the further, the smaller, to make it more realistic\n",
    "\n",
    "\n",
    "\n",
    "**Second mission (optional):**\n",
    "\n",
    "Using the same images segment the cars, motorcycles, trucks and other thermal vehicule and park them properly in the Parking  ('/data/planetary stages/parking.jpg')\n",
    "    bonus:\n",
    "        _- Adapt their orientation to the parking spot_\n",
    "\n",
    "**Last mission (optional):**\n",
    "\n",
    "Make the found traffic lights watch over all the missclassification and other object on the Moon  ('/data/planetary stages/moon.jpg') \n",
    "    \n",
    "- Cars\n",
    "- Pedestrians\n",
    "- Bicycles\n",
    "- Traffic lights \n",
    "- Other object and miss-classification\n",
    "\n",
    "Recompose 3 new images, using the given (or other if you will) background and the segmented instance scatter in the image\n",
    "The new images will present **both** modalities fused (some simple fusion methods are proposed thereafter)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set-up\n",
    "For this mission check how to install and use #YOLO @https://github.com/ultralytics/ultralytics\n",
    "We advice you to use a pretrained version of the model and not train it all by yourself.\n",
    "\n",
    "The documentation for segmentation can be found here : https://docs.ultralytics.com/guides/isolating-segmentation-objects/#how-do-i-isolate-objects-using-ultralytics-yolo11-for-segmentation-tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To facilitate the vizualisation and manipulation of the images / tensor you can download and use this package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ImagesCameras import ImageTensor as im\n",
    "import os\n",
    "import glob\n",
    "from ultralytics import YOLO\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How to use ImageTensor**\n",
    "\n",
    "- Create a new Image : ImageTensor(__source__), where __source__ is np.ndarray, torch.Tensor, str path to file...\n",
    "- Stack 2 images : __image1.hstack(image2)__ or __image1.vstack(image2)__ for respectively horizontal stacking or vertical stacking\n",
    "- Batch 2 images : __image1.batch(image2)__ or __image1.stack(image2)__, batch dimension will be added\n",
    "- Display an image or batched images : __image.show(num='name of the window')__\n",
    "- Change an image Colorspace: __image.COLORSPACE(colormap=None)__, with COLORSPACE in [RGB, GRAY, HSV, HSL, CMYK, XYZ, LAB, LUV, YCbCr], colormap in https://matplotlib.org/stable/gallery/color/colormap_reference.html\n",
    "- print Image properties: __image.pprint()__\n",
    "- crop an image: __image.crop([x, y, h, w], center=False)__, with x, y the anchor point (default is top-left, is center is True, the patch is around the center), h height, w width\n",
    "- Apply a patch on an image: __image.apply_patch(patch, (x, y), in_place=True, center=False, shape=None)__, with a patch (same batch-size and channel) and the anchor point. The flag shape is either a float number to scale the patch or a defined new shape for the patch.\n",
    "\n",
    "For more specific use, ask us it might exist a method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import kornia\n",
    "from kornia.morphology import dilation, erosion\n",
    "from torch import Tensor\n",
    "import numpy as np\n",
    "import torchvision.transforms.functional as TF\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "\n",
    "\n",
    "def fusion_overlapping_bbox(results, *args) -> tuple[Tensor, Tensor]:\n",
    "    masks = results.masks.data.clone().detach()\n",
    "    bbox = results.boxes.xyxy.clone().detach()\n",
    "    if len(args) > 0:\n",
    "        for arg in args:\n",
    "            if masks is not None:\n",
    "                masks = torch.cat([masks, arg.masks.data.clone().detach()], dim=0)\n",
    "                bbox = torch.cat([bbox, arg.boxes.xyxy.clone().detach()])\n",
    "            else:\n",
    "                masks = arg.masks.data.clone().detach()\n",
    "                bbox = arg.boxes.xyxy.clone().detach()\n",
    "\n",
    "    assert masks is not None\n",
    "    length = masks.shape[0]\n",
    "    masks = im(masks, batched=results.masks.data.shape[0]>1)\n",
    "\n",
    "    kernel = torch.ones(5, 5).to(masks.device)\n",
    "    masks_dilated = dilation(masks, kernel)\n",
    "    masks = im(erosion(masks_dilated, kernel))\n",
    "\n",
    "    connectivity = []\n",
    "\n",
    "    for idx, mask in enumerate(masks_dilated[:-1]):\n",
    "        list_idx = (np.argwhere((mask + masks_dilated[idx+1:, 0]).max(-1)[0].max(-1)[0].cpu().numpy()>1)+idx+1).tolist()\n",
    "        list_idx = list_idx[0] if len(list_idx) > 0 else list_idx\n",
    "        connectivity.append(list_idx)\n",
    "    connectivity.append([])\n",
    "    im_fus = []\n",
    "    not_fus = []\n",
    "    for idx in range(length):\n",
    "        c = connectivity[idx]\n",
    "        if len(c) > 0:\n",
    "            new_idx = c.pop(0)\n",
    "            c = [] if c is None else c\n",
    "            if new_idx > idx:\n",
    "                c.append(idx)\n",
    "                connectivity[new_idx].extend(c)\n",
    "            else:\n",
    "                connectivity[new_idx].append(idx)\n",
    "                im_fus.append(connectivity[new_idx])\n",
    "        else:\n",
    "            not_fus.append(idx)\n",
    "\n",
    "    if len(im_fus) > 0:\n",
    "        new_boxes = bbox[not_fus]\n",
    "        list_masks = [masks.extract_from_batch(i) for i in not_fus]\n",
    "        if len(list_masks)>1:\n",
    "            new_mask = im(torch.cat(list_masks, dim=0), batched=len(list_masks)>1)\n",
    "        else:\n",
    "            new_mask = None\n",
    "        for indexes in im_fus:\n",
    "            boxes_fused = torch.tensor([bbox[indexes][:, [0, 2]].min(),  bbox[indexes][:, [1, 3]].min(), bbox[indexes][:, [0, 2]].max(), bbox[indexes][:, [1, 3]].max()])[None]\n",
    "            new_boxes = torch.cat([new_boxes, boxes_fused.to(new_boxes.device)], dim=0)\n",
    "            list_new_masks = [masks.extract_from_batch(i) for i in indexes]\n",
    "            new_mask_add = im(torch.sum(torch.cat(list_new_masks, dim=0), dim=0) > 0, batched=len(list_new_masks)>1)\n",
    "            if new_mask is not None:\n",
    "                new_mask = new_mask.batch(new_mask_add)\n",
    "            else:\n",
    "                new_mask = new_mask_add\n",
    "    else:\n",
    "        new_boxes = bbox\n",
    "        new_mask = masks\n",
    "    return new_boxes, im(new_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "path_data = os.getcwd() + '/data/'\n",
    "images_visible_pathes = sorted(glob.glob(path_data + 'visible/*.jpg'))\n",
    "images_infrared_pathes = sorted(glob.glob(path_data + 'infrared/*.jpg'))\n",
    "\n",
    "images_visible = im.batch(*[im(f) for f in images_visible_pathes])\n",
    "images_infrared = im.batch(*[im(f) for f in images_infrared_pathes])\n",
    "\n",
    "mars = im(path_data + 'planetary stages/mars.jpg')\n",
    "parking = im(path_data + 'planetary stages/parking.jpg')\n",
    "moon = im(path_data + 'planetary stages/moon.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Simple Fusion Methods**\n",
    "\n",
    "Average with colormaped infrared:\n",
    "- Transform the IR grayscale image in a 3 channels image : image_infrared_colored = image_infrared.RGB('gray')\n",
    "- Average the two modalities in a new image : fused_image = image_color/2 + image_infrared_colored/2\n",
    "\n",
    "Average in a new colorspace representation:\n",
    "- Transform the visible image into a new colorspace representation: image_visible_LAB = image_visible.LAB()  (Works also with LUV, HSV, HSL)\n",
    "- Fuse the gray scale images in the L (or V in LUV and HSV) channel: fused_image_LAB = image_visible_LAB // fused_image_LAB[:, 0] = image_visible_LAB[:, 0]/2 + image_infrared[:, 0]/2\n",
    "- Transform back the result into the RGB colorspace: fused_image = fused_image_LAB.RGB()\n",
    "\n",
    "Or propose your own method if you prefer !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# --------------------------------- Image Layout -------------------------------- #\n",
      "Modality: Visible\n",
      "Image size: 1024 x 1280 (height x width)\n",
      "Channel names: Red | Green | Blue\n",
      "Pixel format: RGB | 3 x 8 bits\n",
      "Batch size: 10\n",
      "Layers: batch x channels x height x width || 10 x 3 x 1024 x 1280\n",
      "# -------------------------------------------------------------------------------- #\n",
      "\n",
      "# --------------------------------- Image Layout -------------------------------- #\n",
      "Modality: Any\n",
      "Image size: 1024 x 1280 (height x width)\n",
      "Channel names: Any\n",
      "Pixel format: GRAY | 1 x 8 bits\n",
      "Batch size: 10\n",
      "Layers: batch x channels x height x width || 10 x 1 x 1024 x 1280\n",
      "# -------------------------------------------------------------------------------- #\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualize the data\n",
    "images_visible.pprint()\n",
    "images_infrared.pprint()\n",
    "images_visible.show(num='Visible images')\n",
    "images_infrared.show(num='Infrared images')\n",
    "mars.show()\n",
    "# moon.show()\n",
    "# parking.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First fusion method :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 3, 1024, 1280])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 3, 1024, 1280])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images_infrared_colored = images_infrared.RGB('gray')\n",
    "print(images_infrared_colored.shape)\n",
    "\n",
    "fused_images_1 = images_visible/2 + images_infrared_colored/2\n",
    "\n",
    "fused_images_1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second fusion method :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 3, 1024, 1280])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 3, 1024, 1280])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images_visible_LAB = images_visible.LAB()\n",
    "print(images_visible_LAB.shape)\n",
    "\n",
    "fused_images_LAB = images_visible_LAB\n",
    "\n",
    "fused_images_LAB[:, 0] = images_visible_LAB[:, 0]/2 + images_infrared[:, 0]/2\n",
    "\n",
    "fused_images_2 = fused_images_LAB.RGB()\n",
    "\n",
    "fused_images_2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fused_images_1.show(num='Fused 1 images')\n",
    "fused_images_2.show(num='Fused 2 images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fused_image_1 = im(fused_images_1[0])\n",
    "fused_image_2 = im(fused_images_2[0])\n",
    "\n",
    "\n",
    "fused_image_1.show(num='Fused 1 image')\n",
    "fused_image_2.show(num='Fused 2 image')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard deviation of fused image method 1 : 0.12405599653720856\n",
      "Standard deviation of fused image method 2 : 0.12017931044101715\n",
      "\n",
      "\n",
      "MSE fused image method 1 : 0.05619974061846733\n",
      "MSE fused image method 2 : 0.0647156685590744\n"
     ]
    }
   ],
   "source": [
    "print(f\"Standard deviation of fused image method 1 : {torch.std(fused_images_1)}\")\n",
    "print(f\"Standard deviation of fused image method 2 : {torch.std(fused_images_2)}\")\n",
    "print('\\n')\n",
    "\n",
    "# MSE between a linear combinasion of both visible and infrared images :\n",
    "\n",
    "e_1 = torch.mean( (fused_images_1 - (images_visible + images_infrared_colored))**2 )\n",
    "e_2 = torch.mean( (fused_images_2 - (images_visible + images_infrared_colored))**2 )\n",
    "\n",
    "print(f\"MSE fused image method 1 : {e_1}\")\n",
    "print(f\"MSE fused image method 2 : {e_2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modélisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fused_images_2 = torch.Tensor(fused_images_2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yolo11n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 1024x1280 6 persons, 1 handbag, 324.8ms\n",
      "1: 1024x1280 5 persons, 2 bicycles, 2 cars, 1 motorcycle, 324.8ms\n",
      "2: 1024x1280 7 persons, 1 car, 1 skateboard, 324.8ms\n",
      "3: 1024x1280 4 persons, 5 motorcycles, 1 umbrella, 324.8ms\n",
      "4: 1024x1280 5 persons, 4 traffic lights, 324.8ms\n",
      "5: 1024x1280 5 persons, 3 traffic lights, 324.8ms\n",
      "6: 1024x1280 7 persons, 1 car, 324.8ms\n",
      "7: 1024x1280 (no detections), 324.8ms\n",
      "8: 1024x1280 6 persons, 324.8ms\n",
      "9: 1024x1280 1 person, 1 bicycle, 1 car, 324.8ms\n",
      "Speed: 0.0ms preprocess, 324.8ms inference, 19.0ms postprocess per image at shape (1, 3, 1024, 1280)\n"
     ]
    }
   ],
   "source": [
    "# Load a model\n",
    "model_1 = YOLO('yolo11n.pt')\n",
    "\n",
    "dict_classes = {v: k for k, v in model_1.names.items()}\n",
    "\n",
    "results_1 = model_1.predict(fused_images_2, show=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We can start the segmentation of all the people and bicycles from the different images**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wanted_classes = ['person', 'bicycle', 'motorcycle']\n",
    "\n",
    "classes_to_detect = [v for k, v in dict_classes.items() if k in wanted_classes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_props_1(results, classes=classes_to_detect, confidence_level=0.9) :\n",
    "    \n",
    "    props = []\n",
    "\n",
    "    for res in results :\n",
    "        \n",
    "        img = im(res.orig_img)\n",
    "\n",
    "        img = im(img) \n",
    "            \n",
    "        for boxe in res.boxes :\n",
    "\n",
    "            for xyxy, cls, conf in zip(boxe.xyxy.int(), boxe.cls.int(), boxe.conf) :\n",
    "\n",
    "                if (cls in classes) and (conf.item() > confidence_level) :\n",
    "                        \n",
    "                    x_min, y_min, x_max, y_max = xyxy\n",
    "                    \n",
    "                    cropped_img = img.crop([x_min, y_min, x_max, y_max], xyxy=True)\n",
    "                                \n",
    "                    props.append(cropped_img)\n",
    "                    \n",
    "    print(f\"Number of relevant character detected {len(props)}\")\n",
    "        \n",
    "    return props"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Bon, Yolo11n est pas fait pour la segmentation....."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelisation - Yolo11n-seg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 1024x1280 6 persons, 396.8ms\n",
      "1: 1024x1280 7 persons, 3 bicycles, 2 cars, 1 motorcycle, 396.8ms\n",
      "2: 1024x1280 6 persons, 1 car, 2 skateboards, 396.8ms\n",
      "3: 1024x1280 6 persons, 3 motorcycles, 396.8ms\n",
      "4: 1024x1280 5 persons, 3 traffic lights, 1 umbrella, 2 handbags, 396.8ms\n",
      "5: 1024x1280 2 persons, 3 traffic lights, 396.8ms\n",
      "6: 1024x1280 6 persons, 1 car, 396.8ms\n",
      "7: 1024x1280 (no detections), 396.8ms\n",
      "8: 1024x1280 6 persons, 1 car, 1 traffic light, 396.8ms\n",
      "9: 1024x1280 1 person, 1 bicycle, 1 car, 1 traffic light, 396.8ms\n",
      "Speed: 0.0ms preprocess, 396.8ms inference, 41.9ms postprocess per image at shape (1, 3, 1024, 1280)\n"
     ]
    }
   ],
   "source": [
    "# Load a model\n",
    "model_2 = YOLO('models/yolo11n-seg.pt')\n",
    "\n",
    "dict_classes = {v: k for k, v in model_2.names.items()}\n",
    "\n",
    "results_2 = model_2.predict(fused_images_2, show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_props_2(results, confidence_level=0.8) :\n",
    "    count = 0\n",
    "    props = []\n",
    "    centers = []\n",
    "    \n",
    "    for res in results :\n",
    "        img = np.copy(res.orig_img)\n",
    "        \n",
    "        if res.masks :\n",
    "            for xy, cls, conf, xyxy in zip(res.masks.xy, res.boxes.cls.int(), res.boxes.conf, res.boxes.xyxy) :\n",
    "                \n",
    "                if (cls.item() in classes_to_detect) and (conf.item() > confidence_level) :\n",
    "                    b_mask = np.zeros(img.shape[:2], np.uint8)\n",
    "                    contour = xy.astype(np.int32).reshape(-1, 1, 2)\n",
    "                    cv2.drawContours(b_mask, [contour], -1, (255, 255, 255), cv2.FILLED)\n",
    "\n",
    "                    mask3ch = cv2.cvtColor(b_mask, cv2.COLOR_GRAY2BGR)\n",
    "                    isolated = im(cv2.bitwise_and(mask3ch, img))\n",
    "\n",
    "                    props.append(isolated)\n",
    "                    x_min, y_min, x_max, y_max = xyxy\n",
    "                    centers.append( (int((x_max + x_min) / 2), int((y_max + y_min) / 2)) )\n",
    "                    count += 1\n",
    "                    \n",
    "                    \n",
    "    images = im(torch.stack(props).squeeze(1))\n",
    "\n",
    "    print(f\"Number of relevant props detected : {count}\")\n",
    "    \n",
    "    single_image = im(torch.sum(images, dim=0))\n",
    "\n",
    "    return single_image, images, centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_perspective(images, centers, separation = 3) :\n",
    "    \n",
    "    _, height, width = images[0].shape\n",
    "    process_img = []\n",
    "\n",
    "    seps = [int((height / separation) * i) for i in range(1, separation+1)]\n",
    "\n",
    "    for img, c in zip(images, centers) :\n",
    "    \n",
    "        if (c[1] < seps[0]) :\n",
    "            \n",
    "            img = im(img)\n",
    "\n",
    "            scale_factor = 0.35\n",
    "            \n",
    "            scaling_matrix = torch.tensor([\n",
    "                    [scale_factor, 0, (1 - scale_factor) * width / 2],\n",
    "                    [0, scale_factor, (1 - scale_factor) * height / 2]\n",
    "            ], dtype=torch.float32).unsqueeze(0)\n",
    "            scaled_img = kornia.geometry.transform.warp_affine(\n",
    "            img, scaling_matrix, (height, width), padding_mode=\"zeros\")\n",
    "            \n",
    "            shift_y = int(0.65 * seps[0])\n",
    "            translation_matrix = torch.tensor(\n",
    "            [[1, 0, 0], \n",
    "            [0, 1, shift_y]], \n",
    "            dtype=torch.float32).unsqueeze(0)\n",
    "            translated_img = kornia.geometry.transform.warp_affine(\n",
    "                scaled_img, translation_matrix, (height, width), padding_mode=\"zeros\")\n",
    "\n",
    "            process_img.append(translated_img)\n",
    "            \n",
    "        elif (c[1] > seps[0]) and (c[1] < seps[1]) :\n",
    "            \n",
    "            img = im(img)        \n",
    "            scale_factor = 0.5\n",
    "            scaling_matrix = torch.tensor([\n",
    "                    [scale_factor, 0, (1 - scale_factor) * width / 2],\n",
    "                    [0, scale_factor, (1 - scale_factor) * height / 2]\n",
    "            ], dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "            scaled_img = kornia.geometry.transform.warp_affine(\n",
    "            img, scaling_matrix, (height, width), padding_mode=\"zeros\")\n",
    "            \n",
    "            shift_y = int(0.55 * seps[0])  \n",
    "            translation_matrix = torch.tensor(\n",
    "            [[1, 0, 0], \n",
    "            [0, 1, shift_y]], \n",
    "            dtype=torch.float32).unsqueeze(0)\n",
    "            translated_img = kornia.geometry.transform.warp_affine(\n",
    "                scaled_img, translation_matrix, (height, width), padding_mode=\"zeros\")\n",
    "            process_img.append(translated_img)   \n",
    "                \n",
    "        elif (c[1] > seps[1]) :\n",
    "            \n",
    "            img = im(img)\n",
    "            \n",
    "            scale_factor = 0.8\n",
    "            scaling_matrix = torch.tensor([\n",
    "                    [scale_factor, 0, (1 - scale_factor) * width / 2],\n",
    "                    [0, scale_factor, (1 - scale_factor) * height / 2]\n",
    "            ], dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "            scaled_img = kornia.geometry.transform.warp_affine(\n",
    "            img, scaling_matrix, (height, width), padding_mode=\"zeros\")\n",
    "\n",
    "            shift_y = int((1-scale_factor)  * seps[1])  \n",
    "            translation_matrix = torch.tensor(\n",
    "            [[1, 0, 0], \n",
    "            [0, 1, shift_y]], \n",
    "            dtype=torch.float32).unsqueeze(0)\n",
    "            translated_img = kornia.geometry.transform.warp_affine(\n",
    "                scaled_img, translation_matrix, (height, width), padding_mode=\"zeros\")\n",
    "\n",
    "            process_img.append(translated_img)\n",
    "            \n",
    "    processed_image = im(torch.stack(process_img))   \n",
    "    processed_image = im(torch.sum(processed_image, dim=0))\n",
    "    \n",
    "    return processed_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlay_on_background(foreground_tensor, background_tensor, transparency=0.80):\n",
    "    height, width = foreground_tensor.shape[2], foreground_tensor.shape[3]\n",
    "\n",
    "    background_tensor = TF.resize(background_tensor, (height, width))\n",
    "\n",
    "    foreground = foreground_tensor.squeeze(0)\n",
    "    foreground_np = (foreground.permute(1, 2, 0).cpu().numpy() * 255).astype(np.uint8)\n",
    "    foreground_colored = cv2.applyColorMap(foreground_np, cv2.COLORMAP_OCEAN   ) \n",
    "    foreground_colored = torch.tensor(foreground_colored).permute(2, 0, 1) / 255.0\n",
    "\n",
    "    mask = (foreground < 0.04).all(dim=0)\n",
    "\n",
    "    composite_image = (foreground_colored * transparency + background_tensor * (1 - transparency))\n",
    "    composite_image[:, mask] = background_tensor[:, mask]  \n",
    "    return im(composite_image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of relevant props detected : 21\n"
     ]
    }
   ],
   "source": [
    "image, set_images, centers = retrieve_props_2(results_2)\n",
    "\n",
    "process_img = process_perspective(set_images, centers)\n",
    "\n",
    "result_image = overlay_on_background(process_img, mars.squeeze(0))\n",
    "\n",
    "result_image.show()\n",
    "\n",
    "vutils.save_image(result_image, \"outputs/mars_populated.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
